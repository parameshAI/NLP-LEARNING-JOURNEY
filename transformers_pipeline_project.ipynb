{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# sentiment analysis"
      ],
      "metadata": {
        "id": "QuGSrEqA7Ylq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0c9hyEWuw8k",
        "outputId": "e8e6946b-5531-42a9-bd40-48953b9641b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9997445940971375},\n",
              " {'label': 'NEGATIVE', 'score': 0.9904547333717346}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"Iam becoming a good ai developer\")\n",
        "[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n",
        "classifier(\n",
        "    [\"Iam becoming a good ai developer.\", \"I avoid modern songs!\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#text generation"
      ],
      "metadata": {
        "id": "-u4_I6YJ4jZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In the future, Artificial Intelligence will help humans to\",\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD6cFFyp5YHY",
        "outputId": "34a70b0b-ec9a-4328-b5be-ad9d53d801ff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'In the future, Artificial Intelligence will help humans to develop new technologies.\\nThe next generation of artificial intelligence might be a hybridized AI with human hands and an even more advanced form of computing power that can drive machines across industries (C&O).'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask Filling\n"
      ],
      "metadata": {
        "id": "vmJh_lUb7zMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"my <mask> is paramesh\", top_k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOxPqBaL-BXD",
        "outputId": "d6fc0db8-6cfb-4250-c81f-bd8b1a49ead3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.6819841265678406,\n",
              "  'token': 766,\n",
              "  'token_str': ' name',\n",
              "  'sequence': 'my name is paramesh'},\n",
              " {'score': 0.13509848713874817,\n",
              "  'token': 35807,\n",
              "  'token_str': ' surname',\n",
              "  'sequence': 'my surname is paramesh'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)\n"
      ],
      "metadata": {
        "id": "N0h6NKwS-r4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Paramesh and I am from Tiruchirappalli.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zcTtWb2-8eW",
        "outputId": "d6fa2d80-e5b3-48ca-d648-cc40dfb13593"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': np.float32(0.9922006),\n",
              "  'word': 'Paramesh',\n",
              "  'start': 11,\n",
              "  'end': 19},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': np.float32(0.9799695),\n",
              "  'word': 'Tiruchirappalli',\n",
              "  'start': 34,\n",
              "  'end': 49}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question Answering"
      ],
      "metadata": {
        "id": "ooKp_l5IAmey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I study?\",\n",
        "    context=\"My name is paramesh and I study at st.joseph's college in trichirappali.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OugowJHNAto4",
        "outputId": "e00f6226-3172-4f28-93ea-9c24d7b9c6b5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.4462132155895233,\n",
              " 'start': 35,\n",
              " 'end': 71,\n",
              " 'answer': \"st.joseph's college in trichirappali\"}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Summarization"
      ],
      "metadata": {
        "id": "iVbT32XCBLgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\"\"\"Artificial Intelligence in 2035 requires looking at current trajectories in computing power,\n",
        "              algorithm efficiency, and global investment.By 2035,AI is expected to have transitioned from\n",
        "              a \"tool\" we use occasionally (like a chatbot)to an invisible \"infrastructure\" that runs\n",
        "              the world, much like electricity does today.\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uz7f9dCBUSn",
        "outputId": "61d96b2f-d4ce-46f9-b2a0-bbcb4b828b1a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Your max_length is set to 142, but your input_length is only 110. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ' Artificial Intelligence in 2035 requires looking at current trajectories in computing power, efficiency, and global investment . AI is expected to have transitioned from  a \"tool\" to an invisible \"infrastructure\" that runs like electricity today, much like electricity does today . By 2035,AI will have transitioned to a tool we use occasionally (like a chatbot)'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translation (English to tamil)"
      ],
      "metadata": {
        "id": "myuSB9ZAD4c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-dra\")\n",
        "result = translator(\">>tam<< my name is paramesh \")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wptkh7yYD-a-",
        "outputId": "4ab296b9-29af-4c72-f74d-af030b9855df"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': 'என் பெயர் பரோமஷ்'}]\n"
          ]
        }
      ]
    }
  ]
}
